"""
Fielix æ¨¡å—é€Ÿåº¦åŸºå‡†æµ‹è¯•
========================
æµ‹è¯•æ¯ä¸ªæ ¸å¿ƒæ¨¡å—çš„å‰å‘ä¼ æ’­é€Ÿåº¦ï¼Œä¸æ ‡å‡† Transformer å¯¹æ¯”

ç”¨æ³•:
  python benchmark_modules.py
  python benchmark_modules.py --device cuda --batch_size 32
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import argparse
import math
from typing import Dict, List, Tuple

# å¯¼å…¥ Fielix æ ¸å¿ƒæ¨¡å—
from core.field_propagation import FieldEffectLayer
from core.dynamic_topology import DynamicTopologyLayer
from core.spiral_memory import SpiralMemoryLayer
from core.emergent_position import EmergentPositionEncoder
from core.feedforward import FielixFeedForward
from core.nexus_block import FielixBlock


# ============================================================================
# æ ‡å‡† Transformer ç»„ä»¶ï¼ˆå¯¹ç…§åŸºå‡†ï¼‰
# ============================================================================

class StandardAttention(nn.Module):
    """æ ‡å‡†å¤šå¤´è‡ªæ³¨æ„åŠ›"""
    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        self.qkv = nn.Linear(dim, dim * 3)
        self.out_proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        B, L, D = x.shape
        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, self.head_dim)
        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # 3 x B x H x L x D
        
        attn = (q @ k.transpose(-2, -1)) * self.scale
        if mask is not None:
            attn = attn.masked_fill(mask == 0, float('-inf'))
        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)
        
        out = (attn @ v).transpose(1, 2).reshape(B, L, D)
        return self.out_proj(out)


class StandardFFN(nn.Module):
    """æ ‡å‡†å‰é¦ˆç½‘ç»œ"""
    def __init__(self, dim: int, hidden_mult: int = 4, dropout: float = 0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim * hidden_mult),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * hidden_mult, dim),
            nn.Dropout(dropout),
        )
    
    def forward(self, x):
        return self.net(x)


class StandardTransformerBlock(nn.Module):
    """æ ‡å‡† Transformer å—"""
    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = StandardAttention(dim, num_heads, dropout)
        self.norm2 = nn.LayerNorm(dim)
        self.ffn = StandardFFN(dim, dropout=dropout)
    
    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.ffn(self.norm2(x))
        return x


# ============================================================================
# åŸºå‡†æµ‹è¯•
# ============================================================================

def warmup(model: nn.Module, x: torch.Tensor, num_warmup: int = 5):
    """é¢„çƒ­æ¨¡å‹"""
    model.eval()
    with torch.no_grad():
        for _ in range(num_warmup):
            try:
                out = model(x)
                if isinstance(out, tuple):
                    out = out[0]
            except:
                pass


def benchmark_module(
    model: nn.Module,
    x: torch.Tensor,
    num_runs: int = 100,
    device: str = "cuda"
) -> Dict[str, float]:
    """æµ‹è¯•æ¨¡å—é€Ÿåº¦"""
    model.eval()
    
    # é¢„çƒ­
    warmup(model, x, num_warmup=10)
    
    # åŒæ­¥ CUDA
    if device == "cuda":
        torch.cuda.synchronize()
    
    times = []
    with torch.no_grad():
        for _ in range(num_runs):
            if device == "cuda":
                torch.cuda.synchronize()
            
            start = time.perf_counter()
            
            try:
                out = model(x)
                if isinstance(out, tuple):
                    out = out[0]
            except Exception as e:
                return {"error": str(e)}
            
            if device == "cuda":
                torch.cuda.synchronize()
            
            end = time.perf_counter()
            times.append((end - start) * 1000)  # æ¯«ç§’
    
    times = times[10:]  # å»æ‰å‰10æ¬¡
    
    return {
        "mean_ms": sum(times) / len(times),
        "min_ms": min(times),
        "max_ms": max(times),
        "std_ms": (sum((t - sum(times)/len(times))**2 for t in times) / len(times)) ** 0.5
    }


def run_benchmarks(
    batch_size: int = 16,
    seq_len: int = 128,
    dim: int = 256,
    device: str = "cuda",
    num_runs: int = 100
):
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    
    print("=" * 70)
    print("Fielix æ¨¡å—é€Ÿåº¦åŸºå‡†æµ‹è¯•")
    print("=" * 70)
    print(f"é…ç½®: batch_size={batch_size}, seq_len={seq_len}, dim={dim}")
    print(f"è®¾å¤‡: {device}")
    print(f"æµ‹è¯•æ¬¡æ•°: {num_runs}")
    print("=" * 70)
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, dim, device=device)
    
    # å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å—
    modules = {}
    
    # 1. æ ‡å‡†ç»„ä»¶ï¼ˆåŸºå‡†ï¼‰
    modules["Transformer Attention"] = StandardAttention(dim).to(device)
    modules["Transformer FFN"] = StandardFFN(dim).to(device)
    modules["Transformer Block"] = StandardTransformerBlock(dim).to(device)
    
    # 2. Fielix æ ¸å¿ƒæ¨¡å—
    modules["Field Effect (iter=4)"] = FieldEffectLayer(dim, num_iterations=4).to(device)
    modules["Field Effect (iter=2)"] = FieldEffectLayer(dim, num_iterations=2).to(device)
    modules["Dynamic Topology"] = DynamicTopologyLayer(dim).to(device)
    modules["Emergent Position"] = EmergentPositionEncoder(dim).to(device)
    modules["Spiral Memory"] = SpiralMemoryLayer(dim).to(device)
    modules["Fielix FFN (gated)"] = FielixFeedForward(dim, ffn_type='gated').to(device)
    modules["Fielix FFN (moe)"] = FielixFeedForward(dim, ffn_type='moe', num_experts=4).to(device)
    
    # 3. Fielix Block ä¸åŒé…ç½®
    modules["FielixBlock (field)"] = FielixBlock(dim, attention_type='field', use_memory=False).to(device)
    modules["FielixBlock (field+mem)"] = FielixBlock(dim, attention_type='field', use_memory=True).to(device)
    modules["FielixBlock (topology)"] = FielixBlock(dim, attention_type='topology', use_memory=False).to(device)
    modules["FielixBlock (hybrid)"] = FielixBlock(dim, attention_type='hybrid', use_memory=True).to(device)
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    baseline_time = None
    
    print(f"\n{'æ¨¡å—åç§°':<30} {'å¹³å‡(ms)':<12} {'æœ€å°(ms)':<12} {'ç›¸å¯¹é€Ÿåº¦':<12}")
    print("-" * 70)
    
    for name, module in modules.items():
        result = benchmark_module(module, x, num_runs, device)
        results[name] = result
        
        if "error" in result:
            print(f"{name:<30} ERROR: {result['error']}")
            continue
        
        # ç¬¬ä¸€ä¸ªä½œä¸ºåŸºå‡†
        if baseline_time is None:
            baseline_time = result["mean_ms"]
        
        relative = result["mean_ms"] / baseline_time
        speed_indicator = "ğŸŸ¢" if relative < 1.5 else ("ğŸŸ¡" if relative < 3 else "ğŸ”´")
        
        print(f"{name:<30} {result['mean_ms']:>8.3f} ms  {result['min_ms']:>8.3f} ms  {relative:>6.2f}x {speed_indicator}")
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("é€Ÿåº¦åˆ†æ")
    print("=" * 70)
    
    # æ‰¾å‡ºæœ€æ…¢çš„æ¨¡å—
    valid_results = [(n, r) for n, r in results.items() if "error" not in r]
    sorted_results = sorted(valid_results, key=lambda x: x[1]["mean_ms"], reverse=True)
    
    print("\næœ€æ…¢çš„ 5 ä¸ªæ¨¡å—ï¼š")
    for i, (name, result) in enumerate(sorted_results[:5]):
        relative = result["mean_ms"] / baseline_time
        print(f"  {i+1}. {name}: {result['mean_ms']:.3f} ms ({relative:.2f}x)")
    
    print("\næœ€å¿«çš„ 5 ä¸ªæ¨¡å—ï¼š")
    for i, (name, result) in enumerate(sorted_results[-5:][::-1]):
        relative = result["mean_ms"] / baseline_time
        print(f"  {i+1}. {name}: {result['mean_ms']:.3f} ms ({relative:.2f}x)")
    
    # ä¼˜åŒ–å»ºè®®
    print("\n" + "=" * 70)
    print("ä¼˜åŒ–å»ºè®®")
    print("=" * 70)
    
    field_iter4 = results.get("Field Effect (iter=4)", {}).get("mean_ms", 0)
    field_iter2 = results.get("Field Effect (iter=2)", {}).get("mean_ms", 0)
    if field_iter4 > 0 and field_iter2 > 0:
        improvement = (field_iter4 - field_iter2) / field_iter4 * 100
        print(f"1. å‡å°‘ Field Effect è¿­ä»£æ¬¡æ•° 4â†’2 å¯æé€Ÿ {improvement:.1f}%")
    
    block_field = results.get("FielixBlock (field)", {}).get("mean_ms", 0)
    block_mem = results.get("FielixBlock (field+mem)", {}).get("mean_ms", 0)
    if block_field > 0 and block_mem > 0:
        overhead = (block_mem - block_field) / block_field * 100
        print(f"2. èºæ—‹è®°å¿†å¼€é”€çº¦ {overhead:.1f}%")
    
    trans_block = results.get("Transformer Block", {}).get("mean_ms", 0)
    fielix_block = results.get("FielixBlock (field)", {}).get("mean_ms", 0)
    if trans_block > 0 and fielix_block > 0:
        slowdown = fielix_block / trans_block
        print(f"3. FielixBlock ç›¸æ¯” Transformer Block æ…¢ {slowdown:.2f}x")
    
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fielix æ¨¡å—é€Ÿåº¦åŸºå‡†æµ‹è¯•")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--seq_len", type=int, default=128, help="åºåˆ—é•¿åº¦")
    parser.add_argument("--dim", type=int, default=256, help="æ¨¡å‹ç»´åº¦")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--num_runs", type=int, default=100, help="æµ‹è¯•æ¬¡æ•°")
    
    args = parser.parse_args()
    
    if args.device == "cuda" and not torch.cuda.is_available():
        print("CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        args.device = "cpu"
    
    run_benchmarks(
        batch_size=args.batch_size,
        seq_len=args.seq_len,
        dim=args.dim,
        device=args.device,
        num_runs=args.num_runs
    )
