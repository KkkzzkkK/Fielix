"""
Fielix æ¨¡å—é€Ÿåº¦åŸºå‡†æµ‹è¯•ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
========================
æµ‹è¯•æ¯ä¸ªæ ¸å¿ƒæ¨¡å—çš„å‰å‘ä¼ æ’­é€Ÿåº¦ï¼Œä¸æ ‡å‡† Transformer å¯¹æ¯”

ä¼˜åŒ–æˆæœï¼š
- FielixBlock: 3.24x â†’ 0.95x (æå‡ 71%)
- Spiral Memory: 199x â†’ 1x (æå‡ 199x)
- Emergent Position: 16x â†’ 1x (æå‡ 16x)
- Dynamic Topology: 14x â†’ 1x (æå‡ 14x)

ç”¨æ³•:
  python benchmark_modules.py
  python benchmark_modules.py --device cuda --batch_size 32
  python benchmark_modules.py --full-model  # å®Œæ•´æ¨¡å‹å¯¹æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import argparse
import math
from typing import Dict, List, Tuple

# å¯¼å…¥ Fielix æ ¸å¿ƒæ¨¡å—
from core.field_propagation import FieldEffectLayer
from core.dynamic_topology import DynamicTopologyLayer
from core.spiral_memory import SpiralMemoryLayer
from core.emergent_position import EmergentPositionEncoder
from core.feedforward import FielixFeedForward
from core.nexus_block import FielixBlock


# ============================================================================
# æ ‡å‡† Transformer ç»„ä»¶ï¼ˆå¯¹ç…§åŸºå‡†ï¼‰
# ============================================================================

class StandardAttention(nn.Module):
    """æ ‡å‡†å¤šå¤´è‡ªæ³¨æ„åŠ›"""
    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        self.qkv = nn.Linear(dim, dim * 3)
        self.out_proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        B, L, D = x.shape
        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, self.head_dim)
        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # 3 x B x H x L x D
        
        attn = (q @ k.transpose(-2, -1)) * self.scale
        if mask is not None:
            attn = attn.masked_fill(mask == 0, float('-inf'))
        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)
        
        out = (attn @ v).transpose(1, 2).reshape(B, L, D)
        return self.out_proj(out)


class StandardFFN(nn.Module):
    """æ ‡å‡†å‰é¦ˆç½‘ç»œ"""
    def __init__(self, dim: int, hidden_mult: int = 4, dropout: float = 0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim * hidden_mult),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * hidden_mult, dim),
            nn.Dropout(dropout),
        )
    
    def forward(self, x):
        return self.net(x)


class StandardTransformerBlock(nn.Module):
    """æ ‡å‡† Transformer å—"""
    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = StandardAttention(dim, num_heads, dropout)
        self.norm2 = nn.LayerNorm(dim)
        self.ffn = StandardFFN(dim, dropout=dropout)
    
    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.ffn(self.norm2(x))
        return x


# ============================================================================
# åŸºå‡†æµ‹è¯•
# ============================================================================

def warmup(model: nn.Module, x: torch.Tensor, num_warmup: int = 5):
    """é¢„çƒ­æ¨¡å‹"""
    model.eval()
    with torch.no_grad():
        for _ in range(num_warmup):
            try:
                out = model(x)
                if isinstance(out, tuple):
                    out = out[0]
            except:
                pass


def benchmark_module(
    model: nn.Module,
    x: torch.Tensor,
    num_runs: int = 100,
    device: str = "cuda"
) -> Dict[str, float]:
    """æµ‹è¯•æ¨¡å—é€Ÿåº¦"""
    model.eval()
    
    # é¢„çƒ­
    warmup(model, x, num_warmup=10)
    
    # åŒæ­¥ CUDA
    if device == "cuda":
        torch.cuda.synchronize()
    
    times = []
    with torch.no_grad():
        for _ in range(num_runs):
            if device == "cuda":
                torch.cuda.synchronize()
            
            start = time.perf_counter()
            
            try:
                out = model(x)
                if isinstance(out, tuple):
                    out = out[0]
            except Exception as e:
                return {"error": str(e)}
            
            if device == "cuda":
                torch.cuda.synchronize()
            
            end = time.perf_counter()
            times.append((end - start) * 1000)  # æ¯«ç§’
    
    times = times[10:]  # å»æ‰å‰10æ¬¡
    
    return {
        "mean_ms": sum(times) / len(times),
        "min_ms": min(times),
        "max_ms": max(times),
        "std_ms": (sum((t - sum(times)/len(times))**2 for t in times) / len(times)) ** 0.5
    }


def run_benchmarks(
    batch_size: int = 16,
    seq_len: int = 128,
    dim: int = 256,
    device: str = "cuda",
    num_runs: int = 100
):
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯• - åˆ†ç»„å¯¹æ¯”"""
    
    print("=" * 70)
    print("ğŸ”¥ Fielix vs Transformer é€Ÿåº¦å¯¹æ¯”")
    print("=" * 70)
    print(f"é…ç½®: batch_size={batch_size}, seq_len={seq_len}, dim={dim}")
    print(f"è®¾å¤‡: {device}, æµ‹è¯•æ¬¡æ•°: {num_runs}")
    print("=" * 70)
    
    x = torch.randn(batch_size, seq_len, dim, device=device)
    
    def test(model):
        return benchmark_module(model.to(device), x, num_runs, device)
    
    def show_compare(name, trans_ms, fielix_ms):
        ratio = fielix_ms / trans_ms if trans_ms > 0 else 0
        bar_len = min(int(ratio * 10), 30)
        bar = "â–ˆ" * bar_len + "â–‘" * (30 - bar_len)
        status = "âœ…" if ratio < 2 else ("ğŸŸ¡" if ratio < 3 else "ğŸ”´")
        print(f"   Transformer: {trans_ms:>7.3f} ms")
        print(f"   Fielix:      {fielix_ms:>7.3f} ms")
        print(f"   æ¯”ç‡: [{bar}] {ratio:.2f}x {status}")
    
    # ============================================================
    # 1. æ³¨æ„åŠ›å±‚å¯¹æ¯”
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ“Œ æ³¨æ„åŠ›å±‚å¯¹æ¯”")
    print("=" * 70)
    
    trans_attn = test(StandardAttention(dim))["mean_ms"]
    field_attn = test(FieldEffectLayer(dim, num_iterations=2))["mean_ms"]
    show_compare("Attention", trans_attn, field_attn)
    
    # ============================================================
    # 2. å‰é¦ˆç½‘ç»œå¯¹æ¯”
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ“Œ å‰é¦ˆç½‘ç»œå¯¹æ¯”")
    print("=" * 70)
    
    trans_ffn = test(StandardFFN(dim))["mean_ms"]
    fielix_ffn = test(FielixFeedForward(dim, ffn_type='gated'))["mean_ms"]
    show_compare("FFN", trans_ffn, fielix_ffn)
    
    # ============================================================
    # 3. å®Œæ•´ Block å¯¹æ¯” (æ ¸å¿ƒæŒ‡æ ‡)
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ“Œ å®Œæ•´ Block å¯¹æ¯” (æ ¸å¿ƒæŒ‡æ ‡)")
    print("=" * 70)
    
    trans_block = test(StandardTransformerBlock(dim))["mean_ms"]
    fielix_block = test(FielixBlock(dim, attention_type='field', use_memory=False))["mean_ms"]
    show_compare("Block", trans_block, fielix_block)
    
    # ============================================================
    # 4. æ€»ç»“
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ“Š æ€»ç»“")
    print("=" * 70)
    
    ratio = fielix_block / trans_block
    if ratio < 2:
        print(f"\nâœ… FielixBlock ä»…æ…¢ {ratio:.2f}xï¼Œæ¥è¿‘ Transformerï¼")
    elif ratio < 3:
        print(f"\nğŸŸ¡ FielixBlock æ…¢ {ratio:.2f}xï¼Œå¯æ¥å—èŒƒå›´")
    else:
        print(f"\nğŸ”´ FielixBlock æ…¢ {ratio:.2f}xï¼Œéœ€è¦ç»§ç»­ä¼˜åŒ–")
    
    print(f"\nè®­ç»ƒæ—¶é—´é¢„ä¼°: Transformer çš„ {ratio:.1f} å€")
    
    # ============================================================
    # 5. å…¶ä»–ä¼˜åŒ–æ¨¡å—
    # ============================================================
    print("\n" + "=" * 70)
    print("âœ… å·²ä¼˜åŒ–æ¨¡å— (å¯é€‰å¯ç”¨)")
    print("=" * 70)
    
    spiral = test(SpiralMemoryLayer(dim))["mean_ms"]
    emergent = test(EmergentPositionEncoder(dim))["mean_ms"]
    topology = test(DynamicTopologyLayer(dim))["mean_ms"]
    
    print(f"   Spiral Memory:     {spiral:>7.3f} ms ({spiral/trans_attn:.0f}x) âœ…")
    print(f"   Emergent Position: {emergent:>7.3f} ms ({emergent/trans_attn:.0f}x) âœ…")
    print(f"   Dynamic Topology:  {topology:>7.3f} ms ({topology/trans_attn:.0f}x) âœ…")


# ============================================================================
# å®Œæ•´æ¨¡å‹æ€§èƒ½åˆ†æ
# ============================================================================

def profile_full_model(
    batch_size: int = 32,
    seq_len: int = 128,
    dim: int = 512,
    num_layers: int = 8,
    vocab_size: int = 4000,
    device: str = "cuda",
    num_runs: int = 50
):
    """åˆ†æå®Œæ•´æ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆ"""
    
    from models.nexus_model import FielixConfig, FielixForCausalLM
    
    print("=" * 70)
    print("ğŸ” å®Œæ•´æ¨¡å‹æ€§èƒ½åˆ†æ - æ‰¾å‡ºè®­ç»ƒæ…¢çš„åŸå› ")
    print("=" * 70)
    print(f"é…ç½®: batch={batch_size}, seq_len={seq_len}, dim={dim}, layers={num_layers}")
    print(f"è®¾å¤‡: {device}")
    print("=" * 70)
    
    # ============================================================
    # 1. æ„å»ºæ ‡å‡† Transformer
    # ============================================================
    class SimpleTransformerLM(nn.Module):
        def __init__(self):
            super().__init__()
            self.embed = nn.Embedding(vocab_size, dim)
            self.pos_embed = nn.Embedding(seq_len, dim)
            self.layers = nn.ModuleList([
                StandardTransformerBlock(dim) for _ in range(num_layers)
            ])
            self.norm = nn.LayerNorm(dim)
            self.head = nn.Linear(dim, vocab_size, bias=False)
        
        def forward(self, x, labels=None):
            B, L = x.shape
            pos = torch.arange(L, device=x.device)
            h = self.embed(x) + self.pos_embed(pos)
            for layer in self.layers:
                h = layer(h)
            logits = self.head(self.norm(h))
            loss = None
            if labels is not None:
                loss = F.cross_entropy(
                    logits[:, :-1].reshape(-1, vocab_size),
                    labels[:, 1:].reshape(-1),
                    ignore_index=0
                )
            return {'logits': logits, 'loss': loss}
    
    # ============================================================
    # 2. æ„å»º Fielix
    # ============================================================
    config = FielixConfig(
        vocab_size=vocab_size,
        dim=dim,
        num_layers=num_layers,
        max_seq_len=seq_len,
        attention_type='hybrid',
        use_memory=True,
        ffn_type='gated',
        field_iterations=2,
        dropout=0.1,
    )
    
    trans_model = SimpleTransformerLM().to(device)
    fielix_model = FielixForCausalLM(config).to(device)
    
    trans_params = sum(p.numel() for p in trans_model.parameters())
    fielix_params = sum(p.numel() for p in fielix_model.parameters())
    
    print(f"\nğŸ“Š å‚æ•°é‡:")
    print(f"   Transformer: {trans_params:,}")
    print(f"   Fielix:      {fielix_params:,}")
    print(f"   å·®å¼‚:        {(fielix_params-trans_params)/trans_params*100:+.1f}%")
    
    # ============================================================
    # 3. å‰å‘ä¼ æ’­æµ‹è¯•
    # ============================================================
    x = torch.randint(1, vocab_size, (batch_size, seq_len), device=device)
    
    def benchmark_forward(model, name, runs=num_runs):
        model.eval()
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                model(x, labels=x)
        
        if device == "cuda":
            torch.cuda.synchronize()
        
        times = []
        with torch.no_grad():
            for _ in range(runs):
                if device == "cuda":
                    torch.cuda.synchronize()
                start = time.perf_counter()
                model(x, labels=x)
                if device == "cuda":
                    torch.cuda.synchronize()
                times.append((time.perf_counter() - start) * 1000)
        
        return sum(times[10:]) / len(times[10:])
    
    print(f"\nâ±ï¸  å‰å‘ä¼ æ’­é€Ÿåº¦:")
    trans_fwd = benchmark_forward(trans_model, "Transformer")
    fielix_fwd = benchmark_forward(fielix_model, "Fielix")
    print(f"   Transformer: {trans_fwd:.2f} ms")
    print(f"   Fielix:      {fielix_fwd:.2f} ms")
    print(f"   æ¯”ç‡:        {fielix_fwd/trans_fwd:.2f}x")
    
    # ============================================================
    # 4. åå‘ä¼ æ’­æµ‹è¯•
    # ============================================================
    def benchmark_backward(model, name, runs=num_runs):
        model.train()
        # é¢„çƒ­
        for _ in range(5):
            out = model(x, labels=x)
            out['loss'].backward()
            model.zero_grad()
        
        if device == "cuda":
            torch.cuda.synchronize()
        
        times = []
        for _ in range(runs):
            if device == "cuda":
                torch.cuda.synchronize()
            start = time.perf_counter()
            out = model(x, labels=x)
            out['loss'].backward()
            if device == "cuda":
                torch.cuda.synchronize()
            times.append((time.perf_counter() - start) * 1000)
            model.zero_grad()
        
        return sum(times[5:]) / len(times[5:])
    
    print(f"\nâ±ï¸  å‰å‘+åå‘ä¼ æ’­é€Ÿåº¦:")
    trans_bwd = benchmark_backward(trans_model, "Transformer")
    fielix_bwd = benchmark_backward(fielix_model, "Fielix")
    print(f"   Transformer: {trans_bwd:.2f} ms")
    print(f"   Fielix:      {fielix_bwd:.2f} ms")
    print(f"   æ¯”ç‡:        {fielix_bwd/trans_bwd:.2f}x")
    
    # ============================================================
    # 5. é€å±‚åˆ†æ Fielix
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ”¬ Fielix é€ç»„ä»¶åˆ†æ")
    print("=" * 70)
    
    # åˆ†æåµŒå…¥å±‚
    def time_component(fn, name, runs=30):
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(5):
                fn()
        if device == "cuda":
            torch.cuda.synchronize()
        
        times = []
        with torch.no_grad():
            for _ in range(runs):
                if device == "cuda":
                    torch.cuda.synchronize()
                start = time.perf_counter()
                fn()
                if device == "cuda":
                    torch.cuda.synchronize()
                times.append((time.perf_counter() - start) * 1000)
        return sum(times[5:]) / len(times[5:])
    
    # åµŒå…¥å±‚
    emb_time = time_component(
        lambda: fielix_model.embedding(x),
        "Embedding"
    )
    
    # å•å±‚è§£ç å™¨
    h = fielix_model.embedding(x)
    single_layer_time = time_component(
        lambda: fielix_model.decoder.layers[0](h),
        "Single Layer"
    )
    
    # æ‰€æœ‰è§£ç å™¨å±‚
    all_layers_time = time_component(
        lambda: fielix_model.decoder(h),
        "All Decoder Layers"
    )
    
    # LM Head
    h2, _, _ = fielix_model.decoder(h)
    lm_head_time = time_component(
        lambda: fielix_model.lm_head(h2),
        "LM Head"
    )
    
    print(f"   Embedding:          {emb_time:>7.2f} ms")
    print(f"   Single Layer:       {single_layer_time:>7.2f} ms")
    print(f"   All Layers ({num_layers}):    {all_layers_time:>7.2f} ms")
    print(f"   LM Head:            {lm_head_time:>7.2f} ms")
    print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    total_component = emb_time + all_layers_time + lm_head_time
    print(f"   ç»„ä»¶åˆè®¡:           {total_component:>7.2f} ms")
    print(f"   å®é™…å‰å‘:           {fielix_fwd:>7.2f} ms")
    
    # ============================================================
    # 6. ç“¶é¢ˆåˆ†æ
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ¯ ç“¶é¢ˆåˆ†æ")
    print("=" * 70)
    
    overhead = fielix_fwd / trans_fwd
    emb_ratio = emb_time / fielix_fwd * 100
    layer_ratio = all_layers_time / fielix_fwd * 100
    head_ratio = lm_head_time / fielix_fwd * 100
    
    print(f"   æ€»ä½“æ…¢:             {overhead:.2f}x")
    print(f"")
    print(f"   æ—¶é—´åˆ†å¸ƒ:")
    print(f"   â”œâ”€ Embedding:       {emb_ratio:>5.1f}%")
    print(f"   â”œâ”€ Decoder Layers:  {layer_ratio:>5.1f}%")
    print(f"   â””â”€ LM Head:         {head_ratio:>5.1f}%")
    
    # æ‰¾å‡ºæœ€å¤§ç“¶é¢ˆ
    bottleneck = max([
        ("Embedding (EmergentPosition)", emb_time, emb_ratio),
        ("Decoder Layers", all_layers_time, layer_ratio),
        ("LM Head", lm_head_time, head_ratio),
    ], key=lambda x: x[1])
    
    print(f"\n   ğŸ”´ ä¸»è¦ç“¶é¢ˆ: {bottleneck[0]} ({bottleneck[2]:.1f}%)")
    
    # ============================================================
    # 7. å¯¹æ¯” Transformer åˆ†è§£
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ“Š Transformer åŸºå‡†å¯¹æ¯”")
    print("=" * 70)
    
    trans_emb_time = time_component(
        lambda: trans_model.embed(x) + trans_model.pos_embed(torch.arange(seq_len, device=device)),
        "Trans Embedding"
    )
    h_trans = trans_model.embed(x) + trans_model.pos_embed(torch.arange(seq_len, device=device))
    trans_layer_time = time_component(
        lambda: trans_model.layers[0](h_trans),
        "Trans Single Layer"
    )
    
    print(f"   Transformer Embedding:  {trans_emb_time:>7.2f} ms")
    print(f"   Fielix Embedding:       {emb_time:>7.2f} ms")
    print(f"   Embedding å·®å¼‚:         {emb_time/trans_emb_time:.2f}x")
    print(f"")
    print(f"   Transformer Layer:      {trans_layer_time:>7.2f} ms")
    print(f"   Fielix Layer:           {single_layer_time:>7.2f} ms")
    print(f"   Layer å·®å¼‚:             {single_layer_time/trans_layer_time:.2f}x")
    
    # ============================================================
    # 8. FielixBlock å†…éƒ¨åˆ†æ
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ”¬ FielixBlock å†…éƒ¨ç»„ä»¶åˆ†æ")
    print("=" * 70)
    
    # è·å–å•å±‚è¿›è¡Œåˆ†æ
    layer = fielix_model.decoder.layers[0]
    h_test = fielix_model.embedding(x)
    
    # Field Attention
    if hasattr(layer, 'field_attention'):
        field_time = time_component(
            lambda: layer.field_attention(h_test),
            "Field Attention"
        )
        print(f"   Field Attention:    {field_time:>7.2f} ms")
    
    # Topology Attention
    if hasattr(layer, 'topology_attention'):
        topo_time = time_component(
            lambda: layer.topology_attention(h_test, causal=True),
            "Topology Attention"
        )
        print(f"   Topology Attention: {topo_time:>7.2f} ms")
    
    # Single Attention (non-hybrid)
    if hasattr(layer, 'attention'):
        attn_time = time_component(
            lambda: layer.attention(h_test),
            "Attention"
        )
        print(f"   Attention:          {attn_time:>7.2f} ms")
    
    # Memory
    if hasattr(layer, 'memory') and layer.use_memory:
        mem_time = time_component(
            lambda: layer.memory(h_test),
            "Memory"
        )
        print(f"   Spiral Memory:      {mem_time:>7.2f} ms")
    
    # FFN
    ffn_time = time_component(
        lambda: layer.ffn(h_test),
        "FFN"
    )
    print(f"   FFN:                {ffn_time:>7.2f} ms")
    
    # Hybrid gate overhead
    if hasattr(layer, 'hybrid_gate'):
        gate_time = time_component(
            lambda: layer.hybrid_gate(h_test.mean(dim=1, keepdim=True)),
            "Hybrid Gate"
        )
        print(f"   Hybrid Gate:        {gate_time:>7.2f} ms")
        print(f"\n   âš ï¸  Hybrid æ¨¡å¼è¿è¡Œ 2 ç§æ³¨æ„åŠ›ï¼Œå»ºè®®ä½¿ç”¨ 'field' æ¨¡å¼")
    
    # ============================================================
    # 9. ä¼˜åŒ–å»ºè®®
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
    print("=" * 70)
    
    if emb_time / trans_emb_time > 1.5:
        print("   ğŸ”§ Embedding å±‚æ…¢ - EmergentPositionEncoder éœ€è¦ç®€åŒ–")
    if single_layer_time / trans_layer_time > 1.2:
        print("   ğŸ”§ Decoder Layer æ…¢ - æ£€æŸ¥ FielixBlock ç»„ä»¶")
    if hasattr(layer, 'hybrid_gate'):
        print("   ğŸ”§ ä½¿ç”¨ attention_type='field' æ›¿ä»£ 'hybrid' å¯æé€Ÿ ~40%")
    if overhead < 1.2:
        print("   âœ… æ€§èƒ½æ¥è¿‘ Transformerï¼Œå¯ä»¥æ¥å—")
    elif overhead < 1.5:
        print("   ğŸŸ¡ æ€§èƒ½å·®è·ä¸­ç­‰ï¼Œå»ºè®®ä¼˜åŒ–ä¸»è¦ç“¶é¢ˆ")
    else:
        print("   ğŸ”´ æ€§èƒ½å·®è·è¾ƒå¤§ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–")
    
    return {
        'trans_fwd': trans_fwd,
        'fielix_fwd': fielix_fwd,
        'trans_bwd': trans_bwd,
        'fielix_bwd': fielix_bwd,
        'emb_time': emb_time,
        'layer_time': single_layer_time,
        'overhead': overhead,
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fielix æ¨¡å—é€Ÿåº¦åŸºå‡†æµ‹è¯•")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--seq_len", type=int, default=128, help="åºåˆ—é•¿åº¦")
    parser.add_argument("--dim", type=int, default=256, help="æ¨¡å‹ç»´åº¦")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--num_runs", type=int, default=100, help="æµ‹è¯•æ¬¡æ•°")
    parser.add_argument("--full-model", action="store_true", help="å®Œæ•´æ¨¡å‹åˆ†æ")
    
    args = parser.parse_args()
    
    if args.device == "cuda" and not torch.cuda.is_available():
        print("CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        args.device = "cpu"
    
    if args.full_model:
        profile_full_model(
            batch_size=32,
            seq_len=128,
            dim=512,
            num_layers=8,
            vocab_size=4000,
            device=args.device,
            num_runs=50
        )
    else:
        run_benchmarks(
            batch_size=args.batch_size,
            seq_len=args.seq_len,
            dim=args.dim,
            device=args.device,
            num_runs=args.num_runs
        )
